# Distributed Task Execution Platform
## Web Scraping & Data Collection Platform

A horizontally scalable, distributed web scraping platform built with Python, Celery, RabbitMQ, Redis, and Docker. Process 100k+ scraping jobs/day with intelligent rate limiting, proxy rotation, and real-time monitoring.

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.11+ (for local development)

### Step 1: Clone and Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd DTE-Platform

# Copy environment file
cp env.example .env

# Edit .env file with your configurations (optional for local dev)
```

### Step 2: Start Services with Docker Compose

```bash
# Start all services (PostgreSQL, Redis, RabbitMQ, API)
docker-compose up -d

# Check if all services are running
docker-compose ps
```

### Step 3: Access Services

- **FastAPI API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **RabbitMQ Management UI**: http://localhost:15672
  - Username: `dte_user`
  - Password: `dte_password`
- **PostgreSQL**: localhost:5432
- **Redis**: localhost:6379

### Step 4: Test the API

```bash
# Health check
curl http://localhost:8000/health

# API status
curl http://localhost:8000/api/v1/status
```

---

## ğŸ“ Project Structure

```
DTE-Platform/
â”œâ”€â”€ api/                    # FastAPI application
â”‚   â”œâ”€â”€ main.py            # API entry point
â”‚   â””â”€â”€ routes/            # API routes (to be added)
â”œâ”€â”€ workers/               # Celery workers
â”‚   â””â”€â”€ (to be implemented)
â”œâ”€â”€ storage/               # Database models
â”‚   â””â”€â”€ (to be implemented)
â”œâ”€â”€ config/                # Configuration
â”‚   â””â”€â”€ settings.py       # App settings
â”œâ”€â”€ docker/                # Docker files
â”‚   â””â”€â”€ Dockerfile.api
â”œâ”€â”€ tests/                 # Test files
â”œâ”€â”€ docker-compose.yml     # Docker services
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Development Setup

### Local Development (without Docker for API)

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start services (PostgreSQL, Redis, RabbitMQ)
docker-compose up -d postgres redis rabbitmq

# Run API locally
python -m api.main
# or
uvicorn api.main:app --reload
```

---

## ğŸ“Š Current Status

### âœ… Phase 1: Core Infrastructure (COMPLETED)
- [x] Project structure setup
- [x] Docker Compose configuration
- [x] FastAPI application with health checks
- [x] Configuration management
- [x] PostgreSQL, Redis, RabbitMQ services

### ğŸ”„ Next Steps (Phase 2)
- [ ] Database models and migrations
- [ ] Celery worker setup
- [ ] Job submission API endpoints
- [ ] Basic scraping functionality

---

## ğŸ”§ Configuration

All configuration is managed through environment variables. See `env.example` for available options.

Key configurations:
- **Database**: PostgreSQL connection settings
- **Redis**: Cache and deduplication
- **RabbitMQ**: Message broker for task queues
- **Scraping**: Timeouts, retries, rate limits

---

## ğŸ“ API Endpoints

### Current Endpoints
- `GET /` - Root endpoint
- `GET /health` - Health check
- `GET /api/v1/status` - API status

### Coming Soon
- `POST /api/v1/jobs` - Submit scraping job
- `GET /api/v1/jobs/{job_id}` - Get job status
- `GET /api/v1/jobs` - List all jobs
- `GET /api/v1/stats` - Platform statistics

---

## ğŸ³ Docker Services

The platform runs the following services:

1. **postgres**: PostgreSQL database
2. **redis**: Redis cache
3. **rabbitmq**: RabbitMQ message broker
4. **api**: FastAPI application

---

## ğŸ“š Documentation

- [Technical Specification](./TECHNICAL_SPEC.md) - Complete technical details
- [API Documentation](http://localhost:8000/docs) - Interactive API docs (when running)

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“„ License

MIT License

---

## ğŸ¯ Features (Planned)

- âœ… Basic infrastructure setup
- ğŸ”„ Priority queues (high/normal/low)
- ğŸ”„ Per-domain rate limiting
- ğŸ”„ Proxy rotation
- ğŸ”„ URL deduplication
- ğŸ”„ Auto-retry with exponential backoff
- ğŸ”„ Real-time admin dashboard
- ğŸ”„ Auto-scaling based on queue depth

---

**Status**: Phase 1 Complete - Ready for Phase 2 implementation